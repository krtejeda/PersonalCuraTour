---
title: Usability Testing Checkin
layout: page
---

# Overview


Our first usability test was conducted by two of our team members. One of us solely took notes and recorded the feedback while the other member ran the usability test. We conducted our first usability test on Geneviëve, a studio art major at Williams, in a Schow study room. This participant was ideal because of her visual/aesthetic expertise and willingness to provide constructive feedback. She also spends a lot of time at museums due to her interest in art, so we consider her to be museum adept. The location was ideal because it provided us the freedom to move around and speak freely. 

Our test protocol was as follows. We first introduced ourselves and briefed them on the purpose of this usability test, their role, and our project. We asked if it was okay to collect their feedback via voice recording and note taking and they allowed it. We reassured them that if at anytime they wanted to stop participating, they were free to do so. Next, before we started the testing, we emphasized the importance that she share her thoughts/criticisms about the design of the app, the experience, or anything that wasn’t clear or effective to her. She was made aware that these comments would be used to iterate and improve the design. 

Then we began testing the prototype. She was prompted to imagine that she was in a museum and that she had just opened the app on her watch. We presented her with two tasks: exploring the museum as intended and exploring the museum as they want, and allowed her to explore.  After this point, we stopped giving her further instruction unless she was absolutely stuck, which fortunately never happened. She completed both tasks without any major mishaps, but she did provide feedback through her comments on how the experience could be improved, which we mention later. 

After we finished our usability test, we learned to not underestimate the importance of having smooth and seamless transitions between frames. Our test felt disjointed at some points due to the lag between subsequent frames introduced by physical limitations of our prototype. We felt that this ruined whatever semblance of immersion the user had, and potentially limited the serious feedback they provided. We will organize our frames better so that we can have a smooth testing experience. 

We also learned that it is difficult to elicit the thought processes of the user. They interpreted our desire for audible feedback as having reactionary responses to certain frames, when what we really wanted was the reasoning behind those reactions. We didn’t realize how often we would have to probe the user for more substantive content/feedback. In the future, we will be more clear about the type of feedback that we want by giving an example.

# Cognitive Walkthroughs

![](https://krtejeda.github.io/PersonalCuraTour/img/cogWalkthrough1.jpg)

![](https://krtejeda.github.io/PersonalCuraTour/img/cogWalkthrough2.jpg)

## Issues uncovered during walkthrough

| Photo of Prototype | Description of Incident | Photo of Revision |
| -------- | -------- | -------- |
| A | In our screen for selection of a guided tour, it is not evident that the buttons select for exhibits.  It is not clear from the buttons that what kind of tour is being selected, one that focuses on an exhibit or the entire museum, for example, or even the creator of the tour.  |  X |
| -------- | -------- | -------- |
| B | Need a “no audio” option for those who just want guidance to pieces, without voice-overs playing. | X |
| -------- | -------- | -------- |
| C | There is no option for the user to not rate the tour, but we are not giving users the option to not rate tours in order to force the generation of sufficient feedback | No revision made |
| -------- | -------- | -------- |
| D | The end experience button should be an ‘x’ instead of an arrow, to more graphically correspond with the action being executed | X |
| -------- | -------- | -------- |
| E | The user might want the option to enter the title of the tour differently--for example, to enter the title via voice input.  We will not make this change because it would disrupt the quiet environment of a museum  | No revision made |
| -------- | -------- | -------- |

![](https://krtejeda.github.io/PersonalCuraTour/img/cogWalkthroughImg.jpg)


# Plan for remaining usability tests

Our plan for the remainder of the usability testing is to target several different user demographics as they relate to level of prior experience.  The test we have already done focused on an individual who had a moderate level of experience with museums.  She was a studio art major, implying that she had a higher sense of aesthetics, as stated, but was not highly experienced with museums.  This means that she had valuable insights from the perspective of an individual who has moderate experience with museums, but is not a total expert.  To cover the two other user bases that we are interested in, we will also try to conduct a test on someone who is an expert at museums, or is at least someone who has gone often to many different museums.  This will allow us to hear the views of someone who has lots of prior experience with exhibits, knows the mode of interaction desired, and even potentially has used other types of MuseTech and will be able to make useful contrasts.  Lastly, we will also seek out an inexperienced user to test upon.  This will provide insight with how a user with less preconceptions may view the prototype, and how it may help them relate to art.

The other thing we may look into is changing the test environment itself.  The test we conducted occured in a more casual environment, a Schow study room, without much prior preparation.  This in itself could be improved upon, as we could work to set up a more realistic exhibit-like environment with at least one to two mock pieces that the user could physically transition between, since our application does not exist within a vacuum, but rather a very specific kind of environment.  Further, our application is heavily audio-based; a large portion of the watch interface is used to control the user’s interaction with the audio being played.  This means that the audio we simulate must be at least somewhat realistic in order to engender a believable response from the end user.  Therefore, we will either prepare a sample recording or at least write a script that one of our testers will read off of to simulate operation of this component of our app.  This will ostensibly improve the simulation over the ad-libbing that was done for our first round, which was better than nothing but certainly not quite there.

